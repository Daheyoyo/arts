# ***Algorithm***
## ***26. Remove Duplicates from Sorted Array***
## ***Description***
Given a sorted array nums, remove the duplicates in-place such that each element appear only once and return the new length.

Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory.

### ***Example 1:***

Given nums = [1,1,2],

Your function should return length = 2, with the first two elements of nums being 1 and 2 respectively.

It doesn't matter what you leave beyond the returned length.
### ***Example 2:***

Given nums = [0,0,1,1,1,2,2,3,3,4],

Your function should return length = 5, with the first five elements of nums being modified to 0, 1, 2, 3, and 4 respectively.

It doesn't matter what values are set beyond the returned length.
### My Solution In Java
```
class Solution {
    public void merge(int[] nums1, int m, int[] nums2, int n) {
       int count = m + n - 1;
        m = m - 1;
        n = n - 1;
        while (m >= 0 && n >= 0) {
            nums1[count--] = nums1[m] > nums2[n] ? nums1[m--] : nums2[n--];}
        while (n >= 0) {
            nums1[count--] = nums2[n--];
        }
    }
}
```
### Analyse
My first solution is quite ugly which I don't want to share in here.Then I read the some brief solution on a blog.Here is the thought,the merged array's length is m+n,we can merging  the array  from the largest index(m+n-1) to m or n(depends on the sort).It will be done in the first merging round if  all the elements in nums2 are bigger than which nums1,otherwise we can merging the rest in nums2 in the secoud round.
# ***Review***
### ***Original Artical:***
***https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420***

***https://towardsdatascience.com/a-complete-machine-learning-project-walk-through-in-python-part-two-300f1f8147e2***

***https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-three-388834e8804b***
### ***My Review***
```
This series of articles are good for learning the ABC of machine learning in data science,
by walking through a complete machine learning solution with a real-world dataset to let us see how all the pieces come together 
Let us follow the general machine learning workflow step-by-step:
Data cleaning and formatting
Not every dataset is a perfectly curated group of observations with no missing values or anomalies. 
Real-world data is messy which means we need to clean and wrangle it into an acceptable format before we can even start the analysis.
In short, the goal of EDA is to learn what our data can tell us. It generally starts out with a high level overview, then narrows in to specific areas as we find interesting parts of the data. The findings may be interesting in their own right, or they can be used to inform our modeling choices, such as by helping us decide which features to use.
Data cleaning is an un-glamorous, but necessary part of most actual data science problems.
Exploratory data analysis
Exploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data.
Feature engineering and selection
Feature engineering: The process of taking raw data and extracting or creating new features. This might mean taking transformations of variables, such as a natural log and square root, or one-hot encoding categorical variables so they can be used in a model. Generally, I think of feature engineering as creating additional features from the raw data.
Feature selection: The process of choosing the most relevant features in the data. In feature selection, we remove features to help the model generalize better to new data and create a more interpretable model. Generally, I think of feature selection as subtracting features so we are left with only those that are most important.
Compare several machine learning models on a performance metric

Perform hyperparameter tuning on the best model
In machine learning, after we have selected a model, we can optimize it for our problem by tuning the model hyperparameters.
Controlling the hyperparameters affects the model performance by altering the balance between underfitting and overfitting in a model. 
Evaluate the best model on the testing set
As responsible machine learning engineers, we made sure to not let our model see the test set at any point of training. Therefore, we can use the test set performance as an indicator of how well our model would perform when deployed in the real world.
Making predictions on the test set and calculating the performance is relatively straightforward.
Interpret the model results
The gradient boosted regressor sits somewhere in the middle on the scale of model interpretability: the entire model is complex, but it is made up of hundreds of decision trees, which by themselves are quite understandable. We will look at three ways to understand how our model makes predictions:

Feature importances
Visualizing a single decision tree
LIME: Local Interpretable Model-Agnostic Explainations
The first two methods are specific to ensembles of trees, while the third — as you might have guessed from the name — can be applied to any machine learning model. LIME is a relatively new package and represents an exciting step in the ongoing effort to explain machine learning predictions.
Draw conclusions and document work
At the end of the day, our work is only as valuable as the decisions it enables, and being able to present results is a crucial skill. Furthermore, by properly documenting work, we allow others to reproduce our results, give us feedback so we can become better data scientists, and build on our work for the future.



```

# ***Tip***
```
Last week,my colleague met some performance problem while running some spark job,and he came to my help.I found out the job is running slow because the input hive table had millons' splits when stored in HDFS,which caused the spark job splitted in millons of tasks(partitions) !So,I want to share some tips about spark partitions.
What is Partition in Spark?
In  spark, we store data in the form of RDDs. RDDs refers to Resilient Distributed Datasets. They are a collection of various data items that are so huge in size. That big size data cannot fit into a single node. Thus, we need to divide it into partitions across various nodes, spark automatically partitions RDDs. Also, automatically distributes the partitions among different nodes.

In spark, the partition is an atomic chunk of data. Simply putting, it is a logical division of data stored on a node over the cluster. Partitions are basic units of parallelism and RDDs, in spark are the collection of partitions.
How many partitions should a Spark RDD have?
On the basis of cluster configuration & application, we decide the number of partitions. If we enhance the number of partitions, that make each partition have less data or no data at all. In spark, a single concurrent task can run for every partition of an RDD. Even up to the total number of cores in the cluster.
Best way to decide a number of spark partitions in an RDD is to make the number of partitions equal to the number of cores over the cluster. This results in all the partitions will process in parallel. Also, use of resources will do in an optimal way.
Task scheduling may take more time than the actual execution time if RDD has too many partitions. As some of the worker nodes could just be sitting idle resulting in less concurrency. Therefore, having too fewer partitions is also not beneficial. That may lead to improper resource utilization and also data skewing.  Since on a single partition, data might be skewed.  And a worker node might be doing more than other worker nodes. Hence, when it comes to deciding the number of partitions,  there is always a trade-off.
While a number of partitions are between 100 and 10K partitions. Then based on the size of the cluster and data, the lower and upper bound should be determined.
The lower bond is determined by 2 X number of cores over the cluster.
The upper bound task should take 100+ ms time to execute. If execution time is less than the partitioned data might be too small. In other words, in scheduling tasks application might be spending extra time.
What are Spark Partitioning types?
Hash Partitioning in Spark
partition = key.hashCode () % numPartitions
Range Partitioning in Spark
In some RDDs have keys that follow a particular ordering. Range partitioning is an efficient partitioning technique, for such RDDs. Through this method, tuples those have keys within the same range will appear on the same machine. In range partitioner, keys are partitioned based on an ordering of keys. Also, depends on the set of sorted range of keys.

Both the spark partitioning techniques are ideal for various spark use cases. Yet, the spark still allows users to fine tune by using custom partitioner objects. That how their RDD is partitioned with custom partitioning. Custom partitioning is only available for pair RDDs. Paired RDDs are RDDs with key-value pairs.
```
# ***Share***
```
本周我在阅读耗叔在《程序员练级攻略（2018）：分布式架构经典图书和论文》中推荐的英文书籍<Designing Data-Intensive Applications>，
在这一周中读的速度比较慢，读完了PART I：Foundations of Data Systems >> Chapter I：Reliable, Scalable, and Maintainable Applications
这部分的内容在纸质书上一共25页，PDF上一共47页，内容比较简单，相大家只要接触过各种各样的数据存储引擎的都很容易理解。主要就是给全书后面的内容做一个铺垫，我简单总结一下,以下是我的读书笔记。
当今世界的很多应用都是数据密集型，CPU的性能很少成为瓶颈，反而是大数据量，数据的复杂性和数据变化的速率会引起很多问题。
一个数据密集型的应用通常是为了提供一些目前比较常用的功能，通过一些比较标准的构建模式构建起来的，比如现在很多应用都需要以下几个特点:
    
• 存储数据，以便后续查找 (数据库)
• 记住一些耗时操作的结果，以便下次可以加快查询 (缓存)
• 允许用户通过关键字进行检索或者各种各样的过滤 (搜索引擎)
• 发送消息给另外的进程异步处理 (流式处理)
• 阶段性的对大数据量进行计算 (批处理)
    
由于不同的应用对数据存储有着不同的需求，所以产生了现代的各种各种的有着不同特性的数据存储系统，各式缓存策略，不同的建索引的方式等等。
在构建应用时我们就要想清楚该采用哪种工具哪种策略最合适，有的时候一种工具无法满足需求时可能还得结合各种工具一起使用。
这本书后面的内容就是围绕各种数据存储系统的优缺点，共同点，有哪些地方导致它们有这些差异的，以及如何实现它们各自的特性的等等这些地方进行讨论。
这一章节主要就是讨论这些各式各样的数据存储系统要实现的最基本的目标：可靠的、可伸缩的和可维护的数据存储系统。我们集中讨论软件系统的最重要的三个关注点：
***Reliability(可靠性)***
系统即使在出现硬件错误、软件错误或者人为错误的情况下也应该持续运行正常，在预期的性能下执行正确的功能。一般来说系统只要达到了以下几点就可以说是可靠的：

• 正确执行了功能，返回预期的结果
• 可以容错（硬件错误、软件错误或者人为错误）
• 满足在预期的负载和数据体量下的的性能要求
• 阻止未授权的访问和滥用

***Scalability（可伸缩性）***
随着系统数据体量、数据传输体量以及数据复杂度的增加，负载也会随之增加，应该有合理的方式来处理这些增长。
### 如何描述负载？
能够用来描述增长的因素叫做负载因子，根据系统架构的不同有着不同的最佳负载因子来描述系统增长，有可能是每秒用户请求数，数据库读写频率，聊天室同时在线人数，缓存命中率等等
用推特的案例来描述：在每个用户登录推特主页时，主页要展示他们关注的人在一段时间的推特信息流，技术团队早期的实现方式是在用户请求个人主页信息流时才去查询这些信息流，很艰难地维持这些查询的负载，特别是对于一些名人有着非常多的follower（千万级别）的情况
后面换成了预先计算的方式（提前把每个用户主页信息流统计好然后存储在每个用户的缓存中，类似于邮件收件箱一样的方式）之后，效果就好了很多，因为用户发推的频率（写）比用户查看推特信息流（读）的频率少两个数量级。因此，每个用户的follower的分布情况是非常重要的一个用于描述负载的因子，对于有着大量粉丝名人的推特情况还得单独处理。
### 如何描述性能？
一旦确定了如何描述系统负载，那么你可以观察当负载因子的值增大后会出现什么情况。可以有两个角度来观察，
当系统资源（CPU，内存，网络带宽等）固定时，增大负载因子的值，系统性能是否有影响？
当增加负载因子的值后，要保持系统性能不受影响，需要增加多少系统资源？
以上两个问题都需要依据性能指标，在hadoop这样的批处理系统中我们一般关心系统吞吐量——每秒处理的记录数，或者特定数据量下一个任务运行的总时间，一般的线上系统我们关心服务的响应时间。一个同样的请求我们反复请求，每次的响应时间都可能不一样，造成不一样的原因有很多，进程/线程上下文切换、网络丢包导致TCP协议重传、垃圾回收处理机制、缺页异常导致从读磁盘、服务器机架共振等等都有可能导致随机的延时。
因此在实际中，由于系统同时在处理各种各样的请求，因此响应时间很可能在某一个范围波动，所以我们不应该只关心响应时间这么一个数字，而是响应时间的范围分布，
时间范围的指标可以由算术平均值来描述，当然有时我们关心系统典型的响应时间，平均值就不是一个很好的度量，它没法反应大多数用户的体验。
最好是通过百分位数来表示，通过百分位数更能直观的了解用户的体验情况，比如50%的点是一个中位数，如果中位数的响应时间是200ms，那么说明一半的请求小于200ms，另一半请求超过200ms。
另外，通过百分位数也能更找到那些异常点（outliers），通过查看高百分位数比如p95, p99, and p999的响应，可以知道系统中有多少请求快过在极限情况下的响应，如果p95是1.5s，那么100个请求中有95个请求是小于1.5s的。
对于这些高百分位数区间的响应时间，一般是由于队列延迟（queueing delays）导致的，一个服务器由于受到CPU核数的限制，只能同时并行处理小部分的工作。当一些慢响应的请求占据了CPU资源，阻挡服务器接受随后的请求时，就出现了所谓的头部队列阻塞（head-of-line blocking）效应，即使这些小请求处理起来很快，也要等待之前的请求处理完成，所以在客户端看起来，这些本来很快处理完成的小请求也变得很慢。
因此在人工压测的时候需要模拟这样的场景，通过调整各种度量来缩短等待队列的长度。
另外一种情况也会导致响应时间慢，当需要某几个后台终端调用来处理一个用户请求时，一个很慢的后台终端请求会拖慢整个请求。
### 如何处理负载增大？
一般来说一个系统架构如果能够支撑某一个级别的负载，那么当系统负载增加十倍之后，这个系统是无法支持的。
在设计可伸缩的架构的时候总会在scaling up（垂直拓展，采用更强大的机器）和scaling out（水平扩展，把负载分摊到很多普通机器上）中权衡，
把负载分摊到很多普通机器上又被称为无共享架构（shared-nothing），运行在单一机器上的系统往往更加简单，但是高端的机器通常很昂贵，
因此负载增大之后很难避免采用水平扩展的方式，，但是将系统拆分到多态机器上面运行又会给系统引入一定的复杂度。
大数据体量功能复杂的应用的架构一般都是高度定制化的，没有所谓的万能架构。
数据读写体量，数据存储体量，数据本身的复杂性，响应时间要求，不同的访问策略都会影响可伸缩的架构设计。

***Maintainability（可持续维护性）***
随着时间推移，有各种各样的人来在系统上做一些操作，系统应该便于维护。
大家都知道软件花费精力最多的地方并不是在最开始的开发阶段，而是上线后的长期运维阶段——修复bug、维持系统运行、定位排查问题、适配新平台、为新的用户案例修改加入新特性等等。
相信大家都不愿意去碰一个由于各种历史遗留原因导致非常难以维护的遗留系统，因此在设计系统时就要考虑到系统的可持续维护性，
为了避免让系统变成非常难以维护的遗留系统，一般来说有一下几个设计原则：
### Operability
让运维团队可以方便地去维护系统，优秀的运维团队应该做到以下几点：
• 让运维团队可以方便地去维护系统，优秀的运维团队应该做到以下几点：
• 监控系统健康状态，到服务状态异常时可以快速恢复服务。
• 定位排查导致异常的原因，比如系统失效或者性能降级问题。
• 保持软件和平台都是最新版本，包括一些安全方面的补丁。
• 要清楚各个系统间的依赖关系，每个系统如何影响另外的系统，提前规避可能导致问题的操作。
• 思考以后可能出现的问题，在问题没有出现之前就解决掉。
• 建立使用工具进行部署和配置管理等的最佳实践。
• 可以进行负载的维护工作，比如迁移应用到另一个平台。
• 在系统配置改变时也能随时保持系统的安全性。
• 建立让运维操作的结果可以被预测的流程，保持生产环境稳定。
• 建立系统维护的知识库，在人员变动的情况下也能正常维护系统。
### Simplicity
应该善用抽象，尽量隐藏/降低系统的复杂度，让工程师便于理解。
### Evolvability(Extensibility, Modifiability, Plasticity)
应该让系统可以很方便的加入新特性。
```


